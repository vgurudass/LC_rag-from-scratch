{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a8f42438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain langchain-text-splitters langchain-community bs4 langgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b7d3f547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"false\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = \"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\"\n",
    "os.environ[\"LANGSMITH_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9fdd12",
   "metadata": {},
   "source": [
    "Components\n",
    "We will need to select three components from LangChain’s suite of integrations. Select a chat model: Openai in this case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7dd7f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U \"langchain[openai]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "1fc4be9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "model = init_chat_model(\"gpt-4.1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921a5508",
   "metadata": {},
   "source": [
    "Select an embeddings model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9533c82e",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U \"langchain-openai\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "547d1fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f610e5",
   "metadata": {},
   "source": [
    "Select a vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5924fe49",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U \"langchain-core\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6610bc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "vector_store = InMemoryVectorStore(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea5b08fa",
   "metadata": {},
   "source": [
    "We need to first load the blog post contents. We can use DocumentLoaders for this, which are objects that load in data from a source and return a list of Document objects. In this case we’ll use the WebBaseLoader, which uses urllib to load HTML from web URLs and BeautifulSoup to parse it to text. We can customize the HTML -> text parsing by passing in parameters into the BeautifulSoup parser via bs_kwargs (see BeautifulSoup docs). In this case only HTML tags with class “post-content”, “post-title”, or “post-header” are relevant, so we’ll remove all others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355ac2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "\n",
    "# Only keep post title, headers, and content from the full HTML.\n",
    "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "assert len(docs) == 1\n",
    "\n",
    "print(f\"Total characters: {len(docs[0].page_content)}\")\n",
    "\n",
    "print(docs[0].page_content[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880d67ae",
   "metadata": {},
   "source": [
    "Our loaded document is over 42k characters which is too long to fit into the context window of many models, To handle this we’ll split the Document into chunks for embedding and vector storage. This should help us retrieve only the most relevant parts of the blog post at run time. \n",
    "\n",
    "We use a RecursiveCharacterTextSplitter, which will recursively split the document using common separators like new lines until each chunk is the appropriate size. This is the recommended text splitter for generic text use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6a2a6f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split blog post into 63 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,  # chunk size (characters)\n",
    "    chunk_overlap=200,  # chunk overlap (characters)\n",
    "    add_start_index=True,  # track index in original document\n",
    ")\n",
    "all_splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split blog post into {len(all_splits)} sub-documents.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c131cde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0ac226fd-92a1-4920-85ff-8f58f04dbf27', '8c4c3049-af4e-458a-be35-d0304742e943', 'e06c6c56-278a-4c86-b3a2-cddd6266863a']\n"
     ]
    }
   ],
   "source": [
    "document_ids = vector_store.add_documents(documents=all_splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45364f16",
   "metadata": {},
   "source": [
    "\n",
    "Now let’s write the actual application logic. We want to create a simple application that takes a user question, searches for documents relevant to that question, passes the retrieved documents and initial question to a model, and returns an answer. We will demonstrate:\n",
    "    A RAG agent that executes searches with a simple tool. This is a good general-purpose implementation.\n",
    "    A two-step RAG chain that uses just a single LLM call per query. This is a fast and effective method for simple queries.\n",
    "    \n",
    "One formulation of a RAG application is as a simple agent with a tool that retrieves information. We can assemble a minimal RAG agent by implementing a tool that wraps our vector store:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4b975f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "\n",
    "@tool(response_format=\"content_and_artifact\")\n",
    "def retrieve_context(query: str):\n",
    "    \"\"\"Retrieve information to help answer a query.\"\"\"\n",
    "    retrieved_docs = vector_store.similarity_search(query, k=2)\n",
    "    serialized = \"\\n\\n\".join(\n",
    "        (f\"Source: {doc.metadata}\\nContent: {doc.page_content}\")\n",
    "        for doc in retrieved_docs\n",
    "    )\n",
    "    return serialized, retrieved_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3c50ec",
   "metadata": {},
   "source": [
    "Given our tool, we can construct the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "95e47c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_agent\n",
    "\n",
    "tools = [retrieve_context]\n",
    "# If desired, specify custom instructions\n",
    "prompt = (\n",
    "    \"You have access to a tool that retrieves context from a blog post. \"\n",
    "    \"Use the tool to help answer user queries.\"\n",
    ")\n",
    "agent = create_agent(model, tools, system_prompt=prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d586ba",
   "metadata": {},
   "source": [
    "Let’s test this out. We construct a question that would typically require an iterative sequence of retrieval steps to answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1517590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01fd8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = (\n",
    "    \"What is the standard method for Task Decomposition?\\n\\n\"\n",
    "    \"Once you get the answer, look up common extensions of that method.\"\n",
    ")\n",
    "\n",
    "for event in agent.stream(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": query}]},\n",
    "    stream_mode=\"values\",\n",
    "):\n",
    "    event[\"messages\"][-1].pretty_print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
